import type { RequestHandler } from 'express';
import type { ParamsDictionary } from 'express-serve-static-core';
import type { ParsedQs } from 'qs';
import type { LLMBackend } from '../backends/base';
import type { GenerateRequest } from '../types';
import type { ModelMap } from '../types';

interface RequestBody {
  model: string;
  prompt: string;
  stream?: boolean;
  options?: Record<string, any>;
}

export const handleGenerate = (
  backends: Record<string, LLMBackend>,
  modelMap: ModelMap
): RequestHandler<ParamsDictionary, any, RequestBody, ParsedQs> => {
  return async (req, res, next) => {
    try {
      const { model, prompt, stream = true, options = {} } = req.body;

      console.log('ğŸ¯ ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†é–‹å§‹:', {
        model,
        promptLength: prompt.length,
        stream,
        options
      });

      // ãƒ¢ãƒ‡ãƒ«åãƒãƒƒãƒãƒ³ã‚°
      const key = Object.keys(modelMap)
        .find(k => model.toLowerCase().startsWith(k));
      const target = key ? modelMap[key]
        : { backend: 'openai', model };  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ OpenAI ã«æµã™

      console.log('ğŸ”„ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠ:', {
        requestedModel: model,
        matchedKey: key,
        targetBackend: target.backend,
        targetModel: target.model
      });

      const request: GenerateRequest = {
        model: target.model,
        messages: [{ role: 'user', content: prompt }],
        stream,
        options
      };

      const backend = backends[target.backend];
      if (!backend) {
        res.status(400).json({ error: 'Unsupported backend' });
        return;
      }

      const response = await backend.generate(request);
      
      console.log('âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡:', {
        status: response.status,
        isStream: stream
      });

      res.status(response.status);
      if (stream) {
        response.data.pipe(res);
      } else {
        res.json(response.data);
      }

    } catch (error: unknown) {
      console.error('âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ:', error);
      next(error);
    }
  };
};